# -*- coding: utf-8 -*-
"""Clinical_Trail_Match_gpt3.5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cvuPjolKhk6kMj-90czvYw8qIMEGoer9

### Inference using OpenAI API


1. RAG on Base model (GPT-3.5-turbo)
"""
import random

import openai, os
import pandas as pd
import json
from collections import defaultdict

# For RAG
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma

from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAIEmbeddings

import glob
import PyPDF2
import re

from pathlib import Path
import time

# from openai.cli import display
from pyspark import files

# Upload file containing the OpenAI secret key

with open("./data/trialmatch_jq_key.txt", "r") as file:
    secret_key = file.read().strip()

# Set environment variable
os.environ["OPENAI_API_KEY"] = secret_key

"""### RAG on basemodel"""

llm = ChatOpenAI(temperature=0.1, model_name="gpt-4-1106-preview")


# upload ECOG and Stage context
# ecogstage_context = []
# with open('data/Context/Context/context.txt', 'r') as f:
#     ecogstage_context.append(f.readline())  # ECOG and Stage context


# upload clinical trials pdf

# upload Copy of clinical trials NCCS CTE - updatedDec2023.xlsx
def extract_criteria(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        full_text = ""
        inclusion_criteria = ""
        exclusion_criteria = ""
        found_inclusion = False
        found_exclusion = False

        # Concatenate all pages into one text block
        for page in reader.pages:
            full_text += page.extract_text() + "\n\n"

        # Process the full text
        lines = full_text.split('\n')
        if len(lines) < 100:
            lines = re.split(r"(?<!\d)\.(?!\d)\s", full_text)
        for line in lines:
            if "Inclusion Criteria" in line and not found_inclusion:
                found_inclusion = True
                found_exclusion = False
            elif "Exclusion Criteria" in line and not found_exclusion:
                found_exclusion = True

            if found_inclusion and not found_exclusion:
                inclusion_criteria += line + '\n'
            elif found_exclusion:
                exclusion_criteria += line + '\n'

        return inclusion_criteria.strip(), exclusion_criteria.strip()


documents = []
# for files in os.listdir('data/Context/Context/'):
#     for pdf in files:
#         if pdf.endswith('.pdf'):
#             pdf_name = pdf.split('.pdf')
#             inclusion_criteria_text, exclusion_criteria_text = extract_criteria('./Context/Context/' + pdf)
#             doc_i = Document(page_content=inclusion_criteria_text,
#                              metadata={"source": pdf_name, "criteria": "Inclusion Criteria"})
#             doc_e = Document(page_content=exclusion_criteria_text,
#                              metadata={"source": pdf_name, "criteria": "Exclusion Criteria"})
#             doc_context = Document(page_content=ecogstage_context,
#                                    metadata={"source": pdf_name, "context": "ECOG/Stage Context"})
#
#             documents.append(doc_i)
#             documents.append(doc_e)
#             documents.append(doc_context)

"""### Add webscraped clinical trial data"""

# xls = pd.ExcelFile('./data/trials_set_ner.xlsx')

ct = pd.read_csv('./data/input/example/NCCS_CT dd 07Feb2025.csv')
ct = ct[['Trial Name', 'Study Description', 'Inclusion Criteria', 'Exclusion Criteria']]
# display(ct)
trials_dic = dict()
for i, row in ct.iterrows():
    pdf_name = row['Trial Name']
    print(pdf_name)
    study_description_text, inclusion_criteria_text, exclusion_criteria_text = row['Study Description'], row[
        'Inclusion Criteria'], row['Exclusion Criteria']
    doc_d = Document(page_content=study_description_text,
                     metadata={"source": pdf_name, "criteria": "Study Description"})
    doc_i = Document(page_content=inclusion_criteria_text,
                     metadata={"source": pdf_name, "criteria": "Inclusion Criteria"})
    doc_e = Document(page_content=exclusion_criteria_text,
                     metadata={"source": pdf_name, "criteria": "Exclusion Criteria"})
    # doc_context =  Document(page_content=ecogstage_context, metadata={"source":pdf_name, "context": "ECOG/Stage Context"})

    documents.append(doc_d)
    documents.append(doc_i)
    documents.append(doc_e)
for n, s, i, e in zip(ct['Trial Name'], ct['Study Description'], ct['Inclusion Criteria'], ct['Exclusion Criteria']):
    trials_dic[n.lower()] = "Study Description:" + s + i + e
    print('new trial:', n, '\ndone!')
    # documents.append(doc_context)

"""### Add to vector database"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
all_splits = text_splitter.split_documents(documents)

for i, chunk in enumerate(all_splits):
    if 'criteria' in chunk.metadata.keys():
        all_splits[i].page_content = 'This content is only relevant to the ' + chunk.metadata[
            'criteria'].lower() + ' of clinical trial ' + chunk.metadata['source'] + '. ' + chunk.page_content

embeddings = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])

vectordb = Chroma.from_documents(all_splits, embeddings)

"""### Set up Retriever"""


# Each clinical trial retriever will be filtered with clinical trial name
def rag(qa, query):
    print(f"Query: {query}\n")
    time_1 = time.time()
    result = qa(query)  # qa.run(query)
    time_2 = time.time()
    inf_time = round(time_2 - time_1, 3)
    print(f"Inference time: {inf_time} sec.")
    print("\nResult: ", result)
    return inf_time, result


"""### Retrieve eligibility based on all the clinical trials in the vectordb"""

# modified old ChatGPT prompt shortened for FDA test set
query_template = '''I have a patient profile that needs to be evaluated for eligibility in the clinical trial named {clinical_trial_name}. Please assess the patient's eligibility carefully and methodically. Review each aspect of the patient's profile, including diagnosis, stage, prior therapy, genetic mutations or additional remarks. Determine if each aspect meets the inclusion and exclusion criteria of the clinical trial. If any criterion is not met, stop the evaluation and declare the patient ineligible. If all criteria are met, declare the patient eligible. Please provide a final determination on the patient's overall eligibility for the clinical trial {clinical_trial_name}. Keep your response shorter than 1000 tokens. Here is the patient's profile for your assessment:
{patient_profile}, and here is the trial's information: {trial_info}'''

combined_result = {}

clinical_trials_list = set([e.metadata['source'] for e in all_splits])

user_input = ''


# Batch process ALL clinical trials
def ct_match(clinical_trials_list=clinical_trials_list, user_input=user_input):
    for ct in clinical_trials_list:
        ct_name = ct
        retriever = vectordb.as_retriever(search_kwargs={"k": 7, "filter": {'source': ct_name}})

        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            # retriever=compression_retriever,
            retriever=retriever,
            verbose=True,
            return_source_documents=True
        )
        trial_info = ''
        for k,v in trials_dic.items():
            if ct_name in k.lower():
                trial_info = v
        temp_t, temp_r = rag(qa, query_template.format(clinical_trial_name=ct_name.lower(), patient_profile=user_input, trial_info=trial_info))
        temp_sources = temp_r['source_documents']
        combined_result[ct_name] = {'result': temp_r['result'], 'sources': temp_r['source_documents'],
                                    'pdf': [e.metadata['source'] for e in temp_r['source_documents']], 'time': temp_t}
        page_content = [e.page_content for e in temp_r['source_documents']]
        print(temp_r['result'])
        # a = input()
        return temp_r['result'], temp_r['source_documents'], page_content, temp_t


# ct_match(['D5162C00052','FURMO-004'])

def find_all(string, sub):
    start = 0
    pos = []
    while True:
        start = string.find(sub, start)
        if start == -1:
            return pos
        pos.append(start)
        start += len(sub)


"""### Inference

**Use Case 0 : Return batch results**
"""


# Upload test inference set
def gpt_match(patient_df, patient_number=(random.random() * (10 ^ 6)) // 1, save_path='./data/output/result/OncoPATH_20250303-07/'):
    annotated_profile = patient_df
    patient = annotated_profile['Patient Profile'].values.tolist()
    CTP = annotated_profile['CTP'].values.tolist()
    trials, patient_numbers, per_patient, temps = [], [], [], []
    for i in range(len(patient)):
        single_patient = patient[i]
        trial, per_patient = [], []
        index = find_all(CTP[i], "'")
        id = []
        for b in range(int(len(index) / 2)):
            print(i)
            s = CTP[i][index[2 * b] + 1: index[2 * b + 1]].lower()
            print(s)
            if s is None:
                continue
            else:
                if s in trial:
                    continue
                else:
                    trial.append(s)
        for t in trial:
            per_patient.append(single_patient)
            id.append(str(i))
        print(len(per_patient), len(trial))

        patient_numbers.append(id)
        trials.append(trial)


        temps.append(pd.DataFrame({
            'Patient Profile': per_patient,
            'clinical_trial': trial
        }))

    ind = 0
    for temp in temps:
        print(temp)
        temp.rename(columns={'output': 'Annotated output'}, inplace=True)

        pd.set_option('display.max_colwidth', None)

        # modified old ChatGPT prompt shortened for FDA test set
        prompt_template = '''I have a patient profile that needs to be evaluated for eligibility in the clinical trial named {clinical_trial_name}. Please assess the patient's eligibility carefully and methodically. Review each aspect of the patient's profile, including diagnosis, stage, prior therapy, genetic mutations or additional remarks. Determine if each aspect meets the inclusion and exclusion criteria of the clinical trial. If any criterion is not met, stop the evaluation and declare the patient ineligible. If all criteria are met, declare the patient eligible. Please provide a final determination on the patient's overall eligibility for the clinical trial {clinical_trial_name}. Keep your response shorter than 1000 tokens. Here is the patient's profile for your assessment:
        {patient_profile}, and here is the trial's information: {trial_info}'''

        temp['input'] = temp.apply(
            lambda x: prompt_template.format(clinical_trial_name=x['clinical_trial'],
                                             patient_profile=x['Patient Profile'],
                                             trial_info=x['clinical_trial']),
            axis=1)
        temp['input_length'] = temp['input'].apply(len)
        temp

        import time

        temp[['gpt4_base_RAG_results', 'gpt4_base_RAG_source', 'gpt4_base_RAG_page_content_only', 'query_time']] = temp.apply(
            lambda x: ct_match([x['clinical_trial']], user_input=x['Patient Profile']), axis=1, result_type='expand')

        if not os.path.exists(save_path):
            os.makedirs(save_path)
        temp.to_excel(save_path + 'patent {} match results for gpt4.xlsx'.format(ind), index=False)
        ind = ind + 1


def get_filelist(dir):
    Filelist = []
    for home, dirs, files in os.walk(dir):
        for filename in files:
            # 文件名列表，包含完整路径
            Filelist.append(os.path.join(home, filename))
    return Filelist


if __name__ == "__main__":
    c = input('start query?')
    root_data_path = './data/output/prematch/Prematched OncoPATH_20250310-14_fix_empty.csv'  # './data/input/result'
    # # file_list = get_filelist(root_data_path)
    # #
    # # for i in range(len(file_list)):
    # #     print(i)
    # #     if not file_list[i].endswith('.csv'):
    # #         continue
    #
    df = pd.read_csv(root_data_path)
    gpt_match(df, patient_number=i, save_path='./data/output/result/20250310-14/')

    # for tag in ['TAPISTRY', 'SCG 101-UR-103', 'BAYER 21820', 'EBC-129-01', 'NVL-520-01', 'ALE.CO4.01 ', 'PRT3789-01 ',
    #             'NVL-655-01', 'PAUF-I', 'CAAA603D12101', 'PMV-586-101', 'ZWI-ZW191-01', 'ALE.P02.01',
    #             'TOURMALINE D4191C00140', 'SGNDV-001', 'BEXMET', 'LaPemERLA', 'ELEVATE', 'INAVO122', 'TROPION-Breast04',
    #             'CO44657', 'J2J-MC-JZLH / EMBER-4', 'MK2870-20', '?OTSP-WoOCRC / MK-3475-886', 'MK2870-005',
    #             'MK7902-014', 'ASPEN-06', 'CA209-7R9', 'AHCC09', 'AHCC12', 'SIERRA', 'BO42777 (HORIZON01)',
    #             'BGB-LC-201', 'OSTARA', 'FURMO-004', 'SOS-1 / IIT-SIN-S1-2022-001', 'BI 1479-0008 (BEAMION LUNG-02)',
    #             'HMBD-001-103', 'D5087C00001 SAFFRON', '20230016 DeLLphi-306\r\n', 'VT3996-202', 'MK2140-006', 'POLAR',
    #             'R1979-ONC-2105 OLYMPIA-3', 'R1979-HM-2299 \r\nOLYMPIA-4', 'SG-AAA-II-01 / CAAA601A0SG01T',
    #             'STRO-002-GM2', 'LDOXIRI-PDAC-01', 'CAAA617D12302', 'MK-5684-003', 'MK5684-004', 'XL092-304',
    #             'APL10101 SPARTA', 'TPX-0005-01 TRIDENT-1', 'NCCS-2018-Phase2-PRL3', 'STCC-01', 'MK6482-015']:
    #     retriever = vectordb.as_retriever(search_kwargs={"k": 7, "filter": {'source': tag}})
    #     print(retriever)
